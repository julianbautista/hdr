\chaptertoc{}

\vspace{1em}

At redshifts between 0 and 2, galaxies can be used as tracers of
the matter distribution. With the statistics of the galaxy distribution 
we can measure characteristic scale of the baryon acoustic oscillations (BAO), 
as well as extract information about the growth-rate of structures from 
the anisotropies caused by redshift-space distortions (RSD). 
As presented in Chapter~\ref{chap:intro}, BAO and RSD are powerful 
probes of dark-energy and theories of gravity. 

In this Chapter, I overview my contributions for the study of dark-energy 
with galaxy clustering. Section~\ref{galaxies:catalogue} describes how to 
create a galaxy clustering catalogue from the spectroscopic observations, 
including and correcting for most observational systematic effects.
In section~\ref{galaxies:bao} I present the BAO measurements I performed 
with the SDSS sample of luminous red galaxies, while section~\ref{galaxies:rsd}
focus on the RSD constraints. Section~\ref{galaxies:joint} 
overviews recent work carried out by my PhD students Tyann Dumerchat 
and Vincenzo Aronica on the joint analysis of galaxy clustering in 
Fourier and configuration space. 

The work described in this chapter is published in the following 
articles: 
\cite{bautistaSDSSIVExtendedBaryon2018, 
bautistaCompletedSDSSIVExtended2020,
gil-marinCompletedSDSSIVExtended2020,
rossCompletedSDSSIVExtended2020,
zhaoCompletedSDSSIVExtended2021,
%dumerchatJoint2022
}. 

\section{From redshifts to clustering catalogues}
\label{galaxies:catalogue}

An essential step in the cosmological analysis of galaxy survey data is 
to convert convert a list of galaxy redshifts (see Chapter~\ref{chap:spectro}) 
into a catalogue from which we can define overdensities 
$\delta_n(\vec{x}) = n(\vec{x})/\bar{n} - 1$, where $n(\vec{x})$ is the 
number density 
of galaxies in a volume element located at position $\vec{x}$. 
The quantity $\bar{n}$ is the average galaxy number density over the probed 
\emph{volume}. Therefore, it is important to 1) define precisely what is the 
volume observed, or in jargon terms, the survey window function; and 
2) to make sure that the number density of galaxies is tracing the actual 
cosmological fluctuations and not any spurious contamination. 

The simplest form of survey window function would be some function of position $\vec{x}$ 
that is 1 if the volume was ``observed'' and 0 else. 
One way to define this function is using a Poisson random sampling 
of the volume with points, with some arbitrary higher number density than the 
galaxy average number density (typically between 20 or 50 times higher). 
The list of points is referred to as the \emph{random} catalogue. 
In practice, the random catalogue is more complicated and accounts for 
observational completeness and systematic effects, as described below. 

In SDSS analyses, the random catalogue is a combination of an angular footprint 
and a radial distribution, both trying to matching the galaxy sample as better as possible. 

I describe now the procedure to define the angular footprint. 
The starting footprint is the same where the target selection 
(section~\ref{spectro:target})
was previously defined. 
Figure~\ref{fig:eboss_footprint} displays the eBOSS footprint for the three tracers 
(LRGs, ELGs and QSOs) compared to the BOSS footprint and the stellar density map from Gaia. 
Regions with bad photometric or around 
bright stars are masked out by removing randoms belonging to these regions
(or assigning them a null weight). 
After tiling, fibre assignement and spectroscopic observations, the footprint 
can be divided into an unique set of sectors. Each sector corresponds to regions
observed by one or more plates. The \emph{fibre completeness} of each sector is 
defined as the ratio between the number of spectroscopically observed targets and 
the number of available targets in the sector. Random points are sub-sampled or de-weighted 
following the fibre completeness to account for it. 

\begin{figure}
    \centering 
    \includegraphics[width=0.9\textwidth]{fig/galaxies/eboss_footprint.png}
    \caption{ The DR16 footprint for each eBOSS tracer: Luminous Red Galaxies (LRG), 
    Emission Line Galaxies (ELG) and quasars (QSO). The BOSS DR12 LRGs area is also shown, 
    as well as the density map of Gaia DR2 sources with $g < 15$ mag.
    Figure extracted from \cite{zhaoCompletedSDSSIVExtended2021}.} 
    \label{fig:eboss_footprint}
\end{figure}

Some targets cannot be observed due to 
its proximity to another observed target and the finite size of fibres, corresponding 
to 62 arcseconds in the sky. We refer to these events as \emph{fibre collisions}. 
Collisions might happen not only to pairs of galaxies, but to any group of targets with 
linking lengths smaller than 62 arcsec. Depending on the number of plates observing 
a giving sector, some collisions might be solved but the missing ones might 
impact the measured clustering on small scales if not corrected. 
While there are several methods to solve collisions 
(\cite{guoNewMethodCorrect2012, bianchiUnbiasedClusteringEstimation2017}),
we used the simplest up-weighting technique, where the nearest observed galaxy 
is up-weighted by the number of non-observed targets within the collision group. 
This assumes that non-observed targets are also galaxies of the same target type and 
that they are physically close (angularly and radially) to the observed ones.
On scales above a few \hmpc, this simple correction is a good approximation. 

Two additional sets of weights are defined to correct for spurious density 
fluctuations which contaminate the cosmological fluctuations. 
These weights are applied to the galaxies themselves. 

The first set of weights corrects for angular fluctuations caused 
by correlations between galaxy number density and photometry-related quantities,
such as Galactic extinction, stellar density, imaging depth or sky flux.
They are commonly referred to as \emph{photometric weights}. 
Figure~\ref{fig:photo_systematics_lrg} displays these correlations 
for the LRG sample both before and after corrections. 
For the analysis of eBOSS DR16 LRGs, I have 
implemented\footnote{\url{https://github.com/julianbautista/eboss_clustering/blob/master/python/systematic_fitter.py}} 
a multi-dimension 
linear regression that assumes these correlations are linear and 
accounts for the fact that some photometric quantities are correlated between themselves 
(e.g., stellar density and Galactic extinction).
This multi-linear method is based on work described in \cite{prakashSDSSIVExtendedBaryon2016}.
Of course, more advanced methods have been developed since then, based on 
machine learning algorithms 
(\cite{rezaieImprovingGalaxyClustering2020, chaussidonAngularClusteringProperties2021}).
It is important to evaluate the performance of these algorithms with mock catalogues
where we add artificial contamination 
and quantify how well we recover the initial raw power spectrum after correcting for them. 
There is a risk of overfitting and removing some large-scale cosmological modes which 
can be harmful for clustering analysis, particularly those aiming at measuring 
primordial non-Gaussianity 
(\cite{rezaiePrimordialNonGaussianityCompleted2021, muellerClusteringGalaxiesCompleted2021}).

\begin{figure}
    \centering 
    \includegraphics[width=0.7\textwidth]{fig/galaxies/photometric_weights_lrg.png}
    \caption{ Fluctuations in the angular LRG density as a function of 
    various imaging properties and Galactic foregrounds. 
    The dashed curves show these fluctuations before any correction 
    while red points show the result of applying a linear correction for 
    stellar density and Galactic extinction. 
    Figure extracted from \cite{rossCompletedSDSSIVExtended2020}.} 
    \label{fig:photo_systematics_lrg}
\end{figure}

A second set of weights corrects for the fact for the so-called \emph{redshift failures}, 
i.e., the fact that some spectra with low signal-to-noise ratio (S/N)
do not yield a statistically significant redshift measurement. 
If they were randomly distributed across the sky, failures would not be an issue for 
clustering. However, given the instrument configuration and throughput, these failures 
imprint a pattern in the sky which contaminates clustering. 
In the left panel of Figure~\ref{fig:eboss_zfailures} we see the average failure rate 
of eBOSS LRGs as a function of position in the focal plane. The reason for this pattern
is that the fibres at the side edges of the focal plane transport light to the edges 
of the camera CCDs and therefore have a lower-than-average throughput, or lower signal-to-noise
ratio. 
The right panel of Figure~\ref{fig:eboss_zfailures} shows the 
dependency of the average redshift failure rate as a function of the signal-to-noise
ratio of the observation\footnote{The signal-to-noise ratio of an observation is 
a function of the signal-to-noise ratio of all observed spectra and their magnitudes.}, 
which is the actual underlying reason for the failure rates. 
When modelling these dependencies of failure rates with S/N (or fibre number or focal plane
position), one can correct these spurious fluctuations by assigning a weight 
inversely proportional to the modelled rate. 
These are the so called \emph{redshift-failure weights} and are assigned to the 
galaxies with confident redshift measurements, to compensate for the eventual 
non-confident ones at the same location in the focal plane. 

\begin{figure}
    \centering 
    \includegraphics[width=0.49\textwidth]{fig/galaxies/eboss_z_failures_focalplane.png}
    \includegraphics[width=0.49\textwidth]{fig/galaxies/eboss_z_failures_sn.png}
    \caption{ 
        Average redshift efficiency as a function of physical position of the optical 
        fiber in the focal plane (left panel) and as a function of the observation
        signal-to-noise squared (right panel).  
        The left panel shows how redshift-failures imprint a pattern on the sky 
        and therfore must be corrected. The right panel shows the underlying reason 
        for the failures. The model (red line) is used to compute correction weights.
    Figures extracted from \cite{bautistaSDSSIVExtendedBaryon2018}.} 
    \label{fig:eboss_zfailures}
\end{figure}

Once all galaxies have their correction weights (fibre collision, photometric and 
redshift-failure) and the randoms are corrected by fibre completeness, 
the radial selection function. The idea is to assign redshifts to randoms 
such that they follow the same redshift distribution as the galaxies. 
The redshift distribution is often quantified by the comoving galaxy (weighted)
number density $\bar{n}(z)$. 
Figure~\ref{fig:eboss_z_distrib} shows the comoving density of eBOSS tracers 
as a function of redshift. 
For randoms to match this distribution, we randomly assign the actual galaxy 
redshifts to each random, with repetition. Another possibility is to 
fit some smooth function over the observed $\bar{n}(z)$ and draw random redshifts
from the resulting model. In any case, these methods will introduce the so called 
\emph{radial integral constraint}, i.e., the fact that the observed $\bar{n}(z)$ is 
not the true ensemble averaged function of the Universe, it is just one 
volume-limited realisation of it. The impact on clustering of the radial integral 
constraint is inversely proportional to the area of the footprint
(\cite{demattiaIntegralConstraintsSpectroscopic2019}) and it is significant 
for surveys such as the eBOSS ELG (see Figure~\ref{fig:eboss_footprint} and 
\cite{tamoneCompletedSDSSIVExtended2020, demattiaCompletedSDSSIVExtended2021}). 


\begin{figure}
    \centering 
    \includegraphics[width=0.6\textwidth]{fig/galaxies/eboss_z_distribution.png}
    \caption{ The weighted comoving number densities $\bar{n}(z)$ of eBOSS DR16 tracers
    and BOSS DR12 CMASS LRGs, with all the photometric and spectroscopic
    systematic weights included. 
    The comoving distances and volumes are computed assuming a flat-$\Lambda$CDM 
    cosmology with $\Omega_m = 0.31$. 
    %The three horizontal dashed lines show the number densities of the cubic LRG, ELG, and QSO    
    %EZmockcatalogues,i.e. $3.2 \times 10^{-4}$, $6.4 \times 10^{-4}$, and $2.4 \times 10^{-5} h^3 \text{Mpc}^{-3}$, respectively.
    Figure extracted from \cite{zhaoCompletedSDSSIVExtended2021}.} 
    \label{fig:eboss_z_distrib}
\end{figure}

The final step of the catalogue consists in adding an additional set of weights 
to galaxies that optimise the signal-to-noise ratio of clustering measurements 
when the redshift distribution is not uniform. These weights were first introduced 
by \cite{feldmanPowerSpectrumAnalysisThreedimensional1994} and have been applied 
to all clustering analysis ever since. They are commonly referred to as 
\emph{FKP weights} and can be written for a galaxy $i$ as
\begin{equation}
    w_\text{i, FKP} = \frac{1}{1 + \bar{n}(z_i)P_0}
\end{equation}
where $P_0$ is a rough value of the power spectrum of the tracers at some 
particular scale of choice, usually BAO scales for large-scale clustering measurements.
For eBOSS analyses, we chose a scale of $k \sim 0.1 h\, \text{Mpc}^{-1}$ for which 
$P_0 = 10000 h^{-3}\text{Mpc}^{3}$ for LRGs, 
$P_0 = 4000 h^{-3}\text{Mpc}^{3}$ for ELGs and 
$P_0 = 6000 h^{-3} \text{Mpc}^{3}$ for QSOs. 

As summary, the random catalogue describes the angular and radial selection functions
of the galaxy survey. 
Each random has a weight $w_r = w_\text{comp} w_\text{FKP}$, where $w_\text{comp}$ accounts
the spectroscopic completeness of targets and $w_\text{FKP}$ are optimal weights for clustering. 
Each galaxy has a weight given by $w_g = w_\text{col} w_\text{photo} w_\text{fail} w_\text{FKP}$,
where $w_\text{col}$ accouns for fibre collisions, $w_\text{photo}$ corrects fluctuations caused 
by photometry and $w_\text{fail}$ corrects redshift failures. 
These weights are used computing correlation functions or power spectra (see~\ref{galaxies:clustering}).

\section{Reconstruction of linear density field} 
\label{galaxies:reconstruction}

Bulk motions of galaxies on scales of tens of Mpc impact their two-point statistics, 
acting as a smoothing of the clustering (\cite{eisensteinRobustnessAcousticScale2007}).  
These motions smooth/broaden the BAO peak, reducing the precision on our measurement of its position. 
In the past decades, several methods have been proposed to reconstruct
the linear density field and remove these bulk motions from a galaxy survey,
therefore sharpening the BAO peak.
These methods are referred to as \emph{reconstruction} methods. 
Reconstruction is now an essential part of standard BAO measurements from galaxy surveys,
since they improve significantly their precision. 

The simplest form of reconstruction is based on a theoretical relationship between 
the galaxy density field on large scales (our observable) and the displacement field connecting 
the observed field (Eulerian frame) to its past version (Lagrangian frame). 
Lagrangian perturbation theory (LPT) can predict these displacements 
(see \cite{bernardeauLargeScaleStructureUniverse2002} for a review). 
To first order, the relation between displacements and density is linear 
(\cite{zeldovichGravitationalInstabilityApproximate1970}) and works quite well in 
recovering the linear BAO peak 
(\cite{nusserTracingLargeScaleFluctuations1992, eisensteinImprovingCosmologicalDistance2007}).
The first order LPT reconstruction is often called \emph{Zeldovich reconstruction}. 
Extending to second order LPT does not improve BAO reconstruction significantly 
(\cite{seoHighprecisionPredictionsAcoustic2010}) but it helps on recovering the 
velocity field (\cite{kitauraEstimatingCosmicVelocity2012}).
Zeldovich reconstruction has been used in all BAO measurements from SDSS (BOSS and eBOSS).
I implemented a python version\footnote{\url{https://github.com/julianbautista/eboss_clustering/}} of 
the Zeldovich reconstruction that uses Fast Fourier Transforms while accounting for the 
large angle effects 
(\cite{burdenEfficientReconstructionLinear2014, burdenReconstructionFourierSpace2015}). 
Alternatively, displacements can be computed 
in configuration space using finite difference approximations (\cite{padmanabhanCentDistance352012})
though these are slower and less precise than iterative FFTs. 

Since we do not observe the actual density of matter, an assumed bias relation between galaxy number density 
and matter density has to be assumed in reconstruction. Usually we assume a linear bias relation, where the 
bias value is estimated from the clustering of the standard galaxy catalogue. This is a reasonable approximation 
since reconstruction uses fluctuations on scales larger than $\sim 15$\hmpc. 
If one wants to remove redshifts-space distortions, one has to assume a value for the growth-rate of structures $f$
(see section~\ref{intro:probes:rsd}) in order to convert displacements into velocities. 
Therefore, the clustering of reconstructed catalogues depend on the fiducial values of bias and $f$, 
but less on the distance-redshift relation (\cite{carterImpactFiducialCosmology2020}). 

There are other backward reconstruction techniques such as those based on the least action principle (LAP).
First proposed by \cite{peeblesTracingGalaxyOrbits1989}, it was extended to cosmological applications 
by \cite{nusserLeastActionPrinciple2000, sarpaBAOReconstructionSwift2019}. 
\cite{sarpaExtendedFastAction2021} applied it for the BAO measurement of the BOSS DR12 galaxy sample. 

Reconstruction is essential to improve the precision on BAO measurements. However, 
due to imperfections in recovering the true initial conditions, 
reconstruction has not yet been used in attempts to derive cosmological 
constraints from the full-shape of the reconstructed two-point statistics.
Therefore, analysis of redshift-space distortions are based on the non reconstructed 
clustering (see section~\ref{galaxies:rsd}).  

\section{Mock catalogues}
\label{galaxies:mocks}

An important limitation in cosmology is that we only have a single realisation of our Universe. 
Therefore simulations are an essential tool to overcome this limitation. 

We can recognise two types of simulations mainly used in cosmology. First, 
n-body simulations (eventually including gas) solve numerically for the formation of structures
and are computationally expensive. They focus on the fully non-linear aspects and require a 
higher resolution setting, preventing them to simulate very large volumes or producing numerous 
realisations. N-body simulations are still the baseline for tests of perturbation theory models. 
The second type of simulations are the so-called \emph{mock catalogues} or simply \emph{mocks}. 
They are lower resolution and do not fully describe the non-linear clustering but they are 
faster to produce both in large volumes and in large number of realisations.
Their are key for the understanding of biases in best-fitted quantities and on their uncertainties. 
Given the large number of realisations, mocks have been used to estimate covariance matrices of 
clustering. Also, mocks allow us to study statistically the impact of observational systematic effects on our 
measurements. The last cosmological measurements from galaxy surveys have all an associated set of 
n-body simulations and a set of mock catalogues. I will describe the example I was involved in, 
the case of \textsc{EZmocks} produced for the eBOSS DR16 clustering measurements. 

A set of 1000 \textsc{EZmock} realisations was used in eBOSS DR16 clustering measurements 
for LRGs, ELGs and QSOs. They are fully described in \cite{zhaoCompletedSDSSIVExtended2021}. 
These mocks employ an effective Zeldovich approximation to evolve 
the density field in longer time steps (\cite{chuangEZmocksExtendingZel2015}). 
By construction, these mocks match the large-scale clustering 
of the target data sample, both 2 and 3-point statistics. 
They have some extra free parameters to simulate a biased galaxy sample. 
The eBOSS \textsc{EZmocks} also include light-cone effects produced by interpolating boxes
at different redshifts. 

The last step in the production of \textsc{EZmocks} was the addition of systematic effects. 
I was responsible of implementing all known observational effects from the data. 
A simple script\footnote{\url{https://github.com/julianbautista/eboss_clustering/blob/master/bin/ezmocks_add_systematics.py}}
gathers the information from the real catalogues and simulates fibre collisions, spurious 
fluctuations caused by photometry and spectroscopy (redshift failures). 
These contaminated mocks are then corrected using the same algorithms as for the real data, 
as described in section~\ref{galaxies:catalogue}. The clustering before and after adding 
those effects is extensively compared to real data (\cite{zhaoCompletedSDSSIVExtended2021}) 
and the agreement is found to be excellent on the scales of interest for BAO and RSD measurements. 



\section{From catalogues to clustering estimates}
\label{galaxies:clustering}

Once the catalogue of galaxies and the associated random catalogue are ready, 
any statistical quantity can be computed, such as $n$-point statistics. 
As discussed in section~\ref{intro:lss}, these statistics can be computed 
in either configuration space or Fourier space. 
This section describes how to obtain 2-point function estimates, 
in both spaces, which are the statistics I worked with. 

\subsection{Correlation function}
\label{galaxies:clustering:correlation_function}

The estimate of the correlation function $\xi(\vec{r})$ is based on counting pairs of galaxies 
in bins of separation $\vec{r}$ and comparing it to same numbers obtained from the random catalogue. 
For the estimator we use, there is no need to calculate galaxy number densities $n(\vec{x})$  
or overdensities $\delta_n(\vec{x})$ in some mesh. To optimise the variance of the estimator 
while keeping it unbiased, \cite{landyBiasVarianceAngular1993} proposed to include the cross-pairs  
between galaxies and randoms in the estimator, yielding:
\begin{equation}
 \xi(\vec{r}_i) = \frac{DD(\vec{r}_i) - 2DR(\vec{r}_i)}{RR(\vec{r}_i)} + 1,
 \label{eq:landy-szalay}
\end{equation}
where $DD(\vec{r}_i)$ (or $RR(\vec{r}_i)$) are the normalised paircounts between galaxies (or randoms) 
at bin $i$ corresponding to separation $\vec{r}_i$, 
$DR(\vec{r}_i)$ is the normalised cross-paircounts between galaxies and randoms. 
The normalisation of the paircounts is given by the total number of unique pairs in the catalogue, 
i.e., $N(N-1)/2$ if the catalogue contains $N$ galaxies or $N_g N_r$ for the cross pairs. 
When using weights as described in section~\ref{galaxies:catalogue}, nothing changes except 
that each pair of galaxies (or randoms) are weighted as $w_{ij} = w_i w_j$, where $w_i, w_j$ are the 
weights of each element of the pair. 

More recently, new ways to account for fibre collisions 
or incompleteness have been proposed 
(\cite{bianchiUnbiasedClusteringEstimation2017, percivalUsingAngularPair2017}) where each pair 
of galaxies would have a weight $w_{ij} \neq w_i w_j$, i.e., not a product of individual weights. 
This makes the calculation slighly slower since these weights have to be defined for each pair. 
These new estimators were first used with the eBOSS tracers
(\cite{mohammadCompletedSDSSIVExtended2020}) and are currently being employed within DESI. 

The estimator in Eq.~\ref{eq:landy-szalay} changes slightly when dealing with catalogues 
resulting from reconstruction algorithms,
where both galaxies and randoms are shifted in order to reduce the impact of bulk motions 
(see section~\ref{galaxies:reconstruction}). 
The Landy-Szalay estimator is now written as  
\begin{equation}
    \xi^\text{recon}(\vec{r}_i) = \frac{DD(\vec{r}_i) - 2DS(\vec{r}_i) + SS(\vec{r}_i)}{RR(\vec{r}_i)},
    \label{eq:landy-szalay-recon}
\end{equation}
where $S$ represents the shifted random catalogue and the $R$ is the usual random catalogue as before. 
See \cite{padmanabhanReconstructingBaryonOscillations2009, padmanabhanCentDistance352012} 
for a detailed derivation of this estimator. 

Most analyses of the two-point correlation function do not use the full $\xi(\vec{r}_i)$,
where $\vec{r}_i$ are bins in 2D separations such as bins in $(\rpara, \rperp)$ or $(r, \mu_r)$.
Most analysis further compress the estimated correlation function into a new basis with fewer 
degrees of freedom. Some analyses uses the correlation function 
\emph{wedges} $\xi_{\mu_{r1}, \mu_{r}}(r)$, which are 
averages of $\xi$ over $\mu_{r1} < \mu < \mu_{r2}$, as already discussed in section~\ref{forests:bao:correlations}. 
Other analyses, such as the one described in this chapter, decompose $\xi$ into a basis of 
Legendre polynomials $L_\ell(\mu_r)$ such that $\xi(r, \mu_r) = \sum_\ell \xi_\ell(r) L_\ell(\mu_r)$, 
where $\xi_\ell(r)$ are the correlation function \emph{multipoles}. 
Usually two or three wedges or multipoles are considered when comparing to clustering models.  

One of the most recent codes to compute correlation function accounting for all 
features described above is 
\textsc{pycorr}\footnote{\url{https://github.com/cosmodesi/pycorr}} 
which is being currently developed for DESI.

\subsection{Power spectrum}
\label{galaxies:clustering:power_spectrum}

The power spectrum is the analogous of the correlation function in Fourier space. 
It requires a Fourier Transform of the galaxy overdensity field, which introduces 
some effects that have to be correctly taken into account when fitting for clustering models. 
The power spectrum estimation is based on work by 
\cite{feldmanPowerSpectrumAnalysisThreedimensional1994,
bianchiMeasuringLineofsightdependentFourierspace2015,
handOptimalFFTbasedAnisotropic2017} and it is shortly described here. 

The first step is to assign galaxies and randoms into a three-dimensional mesh, 
defining an ``overdensity'' function $F(\vec{x}_i)$ as 
\begin{equation}
    F(\vec{x}_i) = w_\text{FKP}(\vec{x}_i) \left[ n(\vec{x}_i) - \alpha n_\text{rand}(\vec{x}_i) \right],
    \label{eq:overdensity_fourier}
\end{equation}
where $w(\vec{x}_i)$ is the total weight assigned to voxel $i$ at comoving location 
$\vec{x}_i$, $n$ and $n_\text{rand}$ are the \emph{weighted} number of galaxies and randoms in that voxel, 
$\alpha \equiv \sum_i w_i / \sum_j w^\text{rand}_j$ is the ratio of the total weighted numbers of galaxies to 
randoms. 

The field $F(\vec{x})$ can be Fourier transformed numerically using Fast Fourier Transform (FFT) 
algorithms. The power spectrum would be defined as the absolute value squared of this field. 
Power spectrum \emph{multipoles} are commonly 
used, as it was the case in configuration space. 
They can be expressed as: 
\begin{equation}
 P_\ell(k) = \frac{1}{I_{22}} \left[ (2\ell+1) \left\langle F_0(\vec{k})F_\ell(-\vec{k}) \right\rangle_{V_k} - (1+\alpha)I_{12} \right]
 \label{eq:power_spectrum_multipoles}
\end{equation}
where the brakets $\langle \cdot \rangle$ indicate average over spherical shells in k-space with radius $|\vec{k}|$, 
$F_\ell(\vec{k})$ are the overdensity multipoles in k-space given by 
\begin{equation}
    F_\ell(\vec{k}) = \int \text{d}^3x F(\vec{x}) \text{e}^{i \vec{k}\cdot \vec{r}} L_\ell( \hat{k} \cdot \hat{x}), 
\end{equation}
and the normalisation factors $I_{ab}$ are defined as 
\begin{equation}
 I_{ab} \equiv \int \text{d}^3 x [n(\vec{x})]^a [w_\text{FKP}(\vec{x})]^b. 
 \label{eq:normalisation_power_spectrum}
\end{equation}

The first term in Eq.~\ref{eq:power_spectrum_multipoles} is the usual term defining the power spectrum, i.e., 
the square of the absolute value of Fourier fluctuations, while the second term is the so called \emph{shot-noise} 
and only affects the power spectrum monopole. 

Since we use Fourier transforms which rely on periodic boundary conditions, the mesh used to compute $F(\vec{x})$
has to be large enough to encompass the survey and should contain some padding around it. 
Also, because of the assumption of periodicity, the estimated power spectra will be convolved by the window/selection function of the survey.
This window has to be correctly computed using the randoms and it is a key ingredient to be accounted for when modelling the clustering. 

Since we use numerical FFTs, the resolution of the mesh defines the amount of memory to be used. 
Typically resolutions of 512 or 1024 mesh voxels on a side are sufficient to cover the scales of interest 
for BAO or RSD analyses. If the mesh has a total comoving size of $L$ on a side and has $N$ voxels on a side, 
the fundamental mode is given by $k_F = \pi/L$ while the highest (or Nyquist) frequency is $k_\text{Ny} = \pi N/L$. 

The estimated power spectra are also affected by the discreteness nature of the mesh on scales close 
the Nyquist frequency. The assignement of galaxies to the mesh can be optimised to reduce those effects. 
Also, interlacing techniques can also further reduce mesh effects. T
The effects and solutions are described in detail in \cite{sefusattiAccurateEstimatorsCorrelation2016}
and are implemented in the package \textsc{nbodykit}\footnote{\url{https://nbodykit.readthedocs.io/}}
or \textsc{pypower}\footnote{\url{https://github.com/cosmodesi/pypower}}. 

\subsection{The clustering of eBOSS DR16 LRGs}
\label{galaxies:clustering:eboss_dr16_lrgs}

Figure~\ref{fig:LRG_data_ezmock_correlations_prerec} and \ref{fig:LRG_data_ezmock_correlations_postrec}
show the power spectrum $P_\ell(k)$ and correlation function $\xi_\ell(r)$ multipoles for $\ell = 0, 2, 4$
for the combined sample of BOSS DR12 and eBOSS DR16 luminous red galaxies and their respective \textsc{EZmock}
catalogues. Figure~\ref{fig:LRG_data_ezmock_correlations_prerec} shows results from standard catalogues while 
Figure~\ref{fig:LRG_data_ezmock_correlations_postrec} displays results from reconstructed catalogues. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/galaxies/DR16LRG_data_ezmock_prerecon.pdf}
    \caption{Pre-reconstruction multipoles of the two-point statistics for the combination of BOSS DR12 and 
    eBOSS DR16 luminous red galaxies. Left panels contain the power spectrum multipoles (scaled by $k$) 
    and right panels the correlation function multipoles (scaled by $r^2$). 
    Points with errorbars are real data while shaded regions show the mean and standard deviation 
    of 1000 realisations of \textsc{EZmocks}. 
    }
    \label{fig:LRG_data_ezmock_correlations_prerec}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{fig/galaxies/DR16LRG_data_ezmock_postrecon.pdf}
    \caption{Same as Figure~\ref{fig:LRG_data_ezmock_correlations_prerec} but for reconstructed 
    catalogues. We used the Zeldovich reconstruction method with removal of redshift-space distortions. 
    The BAO features are more proeminent and the quadrupole is significantly reduced compared 
    to results with the standard catalogues.    
    }
    \label{fig:LRG_data_ezmock_correlations_postrec}
\end{figure}

The uncertainties on our measurements of $P_\ell$ and $\xi_\ell$ were derived by measuring the 
same statistics on the full sample of 1000 realisations of \textsc{EZmocks}. By reproducing with 
high fidelity the observational features and systematic effects, the covariance matrix obtained 
with mocks should account for the extra scatter induced by them. However on small scales the 
clustering of the mocks is not accurate compared to real data or to more realistic n-body simulations. 
Figure~\ref{fig:covariance_ezmock} shows the resulting covariance matrices normalised by the 
diagonal elements (correlation coefficients). We see how power spectra multipoles are generally 
less correlated than correlation function multipoles. Reconstruction also helps reducing these 
correlations. Also shown are the cross-covariance between Fourier space and configuration space 
measurements, which will be discussed in section~\ref{galaxies:joint}. As expected, these
two spaces are highly correlated since they originate from the same sample of galaxies and randoms. 

\begin{figure}
    \centering 
    \includegraphics[width=0.49\textwidth]{fig/galaxies/covariance_prerecon.pdf}
    \includegraphics[width=0.49\textwidth]{fig/galaxies/covariance.pdf}
    \caption{Correlation coefficients from covariance matrices obtained from 1000 measurements
    of power spectrum and correlation function multipoles, $P_\ell$ and $\xi_\ell$ respectively,
    from \textsc{EZmocks} reproducing the combined BOSS DR12 and eBOSS DR16 LRG sample.
    Left panel shows results from the standard catalogues while the right panel 
    shows the results from Zeldovich reconstructed catalogues. 
    }
    \label{fig:covariance_ezmock}
\end{figure}

\section{Baryon acoustic oscillations}
\label{galaxies:bao}

Model 

Constraints with DR16 

\section{Redshift-space distortions}
\label{galaxies:rsd}

Model 

Constraints with DR16 

\section{Joint clustering analysis in Fourier and Configuration space}
\label{galaxies:joint}




\section{Cross-correlation with radio surveys}
\label{galaxies:radio}

\cite{wolzHIConstraintsCrosscorrelation2021}