\chaptertoc{}

\vspace{1em}



\section{The history of our Universe and its big open questions}
\label{intro:history_questions}

    If we gather all knowledge in physics humanity could learn to this day, 
    we can build an interesting story for how our Universe evolved since 
    \emph{very} early times. This great story that can explain most of what 
    we observe in the sky, while being consistent with experimental results here on Earth.  
    However, this story has several weak spots. Either because where we do not have 
    a satisfactory physical explanation for what we observe, or 
    because we simply cannot observe what happens by then (and potentially we never will).

    The story in a nutshell goes as follows: the ``beginning''
    of the Universe\footnote{ The actual beginning of 
    the Universe can be seen as a complex phylosophical question or simply as an 
    ill defined physical concept. This discussion is clearly beyond the scope of this work 
    and/or my capabilities. Here, I decide to name as beginning the first epoch of the Universe
    for which we have a widely known physical theory to describe it.}, 
    also known as the \emph{Big Bang} or \emph{inflation}, was an epoch of exponential 
    expansion of space that happened around 13.8 billion years ago. 
    Inflation transformed quantum, microscopic fluctuations of space
    into macroscopic density fluctuations of the Universe's constituents:
    quarks, photons, neutrinos and dark-matter. 
    After inflation, the Universe continues to expand but much slowly. 
    The average temperature of the Universe decreases adiabatiacally. 
    Quarks start to gather to form protons, neutrons and relics of heavier atoms, 
    such as deuterium, tritium, helium, litium and so on. 
    All this during the first few minutes of the Universe's history, in the so called
    \emph{Big Bang nucleosynthesis}. 
    After 380 thousand years, electrons rebind with protons to form neutral 
    hydrogen, and photons scatter for their last time with them as the Universe becomes
    more difuse. 
    The photons from this epoch are observable today and are known as the 
    \emph{cosmic microwave background}. 
    The Universe goes through its \emph{dark ages}, where hydrogen is mostly neutral and 
    stars have not yet formed. 
    After several million years, gravity clumps hydrogen into dense clouds that reach 
    high enough temperatures to start thermonuclear reactions in their core. 
    The first stars are formed. Then the first galaxies. These galaxies merge into larger 
    galaxies, galaxies form small virialized groups, large groups, clusters, super clusters. 
    Clusters, filamentary structures and voids compose the so called cosmic web. 
    At around half of the Universe's age, the expansion starts to speed up again,
    accelerating, as if gravity became repulsive on very large scales. 
    Then, on one of the many billions of galaxies, the \emph{Milky Way}, 
    planet Earth formed around a somewhat isolated star, the Sun, 
    and here we are today, observing the sky, trying to explain the whole Universe. 

    This story is based on the assumption that general relativity (GR) is the theory of gravity, 
    that the Universe can be modelled as statistically homogeneous and isotropic on large enough
    scales, and that the Universe is composed of the standard model particles and interactions, 
    with the addition of two exotic components: dark-energy and dark-matter. 

    The physical origin of dark-energy and dark-matter is a mystery and one of the weak spots 
    of the Universe's story. Another weak spot is the physics of inflation, but observables 
    related to that time are limited. [Any others ?] Solutions for these problems 
    would represent major breakthroughts in physics and large teams of scientists are dedicated
    to them, either theoretically or experimentally. 

    My past work has been focused on the observations related to dark-energy. 
    The rest of this chapter will be dedicated to explaining the problem 
    of accelerated expansion, the dark-energy model and other possible solutions,
    and how do we observe the expansion. 

\section{Evidences for the accelerated expansion of the Universe}
\label{intro:evidences_acceleration}

    Not long after developing his theory of gravity, Einstein wanted to write a model 
    for a static Universe filled with matter. Since gravity is attractive, his solution 
    was to include a new constant term, $\Lambda$, in the equations. This will be later
    known as the \emph{cosmological constant}, one of the simplest models of dark-energy
    in agreement with a wide variety of cosmological observations. 
    
    The first indirect evidences for dark-energy come from measurements of galaxy clustering in 
    the 80's (\cite{maddoxGalaxyCorrelationsLarge1990, 
    efstathiouCosmologicalConstantCold1990}). 
    At the end of the 90's, two independent teams measured the expansion of 
    the Universe using type-Ia supernovae 
    (\cite{riessObservationalEvidenceSupernovae1998, perlmutterMeasurementsOmegaLambda1999}). 
    Their observations could only be explained by 
    an Universe containing around 30 per cent of matter and 70 per cent of dark energy,
    in the form of a cosmological constant. Few years later, first measurements of the 
    temperature fluctuations in the cosmic microwave background were consistent with 
    an Universe with a flat space (\cite{balbiConstraintsCosmologicalParameters2000,
    debernardisFlatUniverse2000}), 
    which in combination with measurements of clustering, 
    supernovae and the local expansion rate (\cite{mouldHubbleSpaceTelescope2000}), 
    definitely confirmed that the Universe's expansion is accelerating. 
    Age estimates of globular stellar clusters, which are
    supposed to be among the oldest astrophysical objects, were also indicating that 
    the Universe had to be older than the age predicted by models of an Universe only 
    filled with matter (\cite{chaboyerAgeUniverse1998}). 
    The following decade was enriched by the first measurements
    of the baryon acoustic oscillations in the distribution of galaxies
    (\cite{eisensteinDetectionBaryonAcoustic2005, cole2dFGalaxyRedshift2005}),
    re-confirming the accelerated nature of the expansion.
    
    Since the 2010's, we live in the so-called \emph{precision cosmology} era,
    where several observatories were constructed with the main goal of
    improving the precision of all these measurements just discussed. 
    Even with the most up-to-date results, with errors of the order of a few percent,
    there is yet no evidence for a departure from the model of Universe 
    governed by GR, composed by 70 per cent of this mysterious cosmological 
    constant and by 25 per cent by this mysterious dark-matter. 

\section{Model of an expanding Universe}
\label{intro:model}

    \subsection{Assumptions and ingredients}
    \label{intro:model:ingredients}

    In order to define some important cosmological parameters, I will quickly review
    the basics of the current most accepted model for our Universe in the limit of 
    very large scales, where it can be considered homogenous and isotropic. 
    This is also knwon as the \emph{background} model, since the formation of structures 
    (see next section) can
    be modeled as small perturbations around it. It is also assumed that gravity is 
    described by general relativity, which interlinks the fabric of space-time and 
    energy densities of the Universe constituents. For these constituents, we assume 
    the presence of \emph{baryons} (protons, electrons, atoms), 
    photons, neutrinos, cold dark-matter and dark-energy. 

    The expansion of space is usually parametrised by the scale factor $a(t)$, 
    a function of time $t$ that dictates how physical and comoving distances 
    relate to each other. We assume that $a(t_0) = 1$ today ($t_0 = 13.8$ Gyr) 
    and that $a \rightarrow 0$ as $t \rightarrow 0$. The speed of the expansion
    and its acceleration are simply derivatives with respect to time, $\dot{a}(t)$ 
    and $\ddot{a}(t)$, respectively. The expansion rate of the Universe is defined
    as $H(t) = \dot{a}(t)/a(t)$ and takes the Hubble constant value today, 
    $H(t_0) = H_0 = 100 h ~ \mathrm{km/s/Mpc} \sim 70$~km/s/Mpc. 
    Since the scale factor is a monotonically increasing function of time 
    (in our simple model), $a$ is commonly used to describe a given cosmic epoch. 

    One of the consequences of the expansion is that relativistic species, such 
    as photons and massless neutrinos, loose energy or are \emph{redshifted} as they 
    propagate. In astronomy, the redshift $z$ is defined as the relative difference
    in wavelength between the one emmited by source and the one observed.  
    Thus, in an expanding Universe the scale factor is related to the redshift as 
    $a = 1/(1+z)$. Redshift is also commonly used to describe cosmic epoch or distances.
    Today corresponds to $z=0$ while $z \rightarrow \infty$ as $t \rightarrow 0$. 

    \subsection{The expansion rate}
    \label{intro:model:expansion_rate}

    The expansion rate can be derived from GR equations and be written as a function of 
    redshift and the densities of each consituent
    \begin{equation}
        H^2(z) = H^2_0 \left[ \Omega_m(1+z)^3 + \Omega_r (1+z)^4 + \Omega_{de}(z) + \Omega_k(1+z)^2\right]
        \label{eq:expansion_rate}
    \end{equation}
    where $\Omega_x = \rho_x / \rho_\mathrm{ crit}$ is the ratio between the density 
    constituent $x$ today and the critical energy density $\rho_\mathrm{ crit} = 3H_0^2/8\pi G$, 
    which is the density required for a flat space geometry. 
    The subscripts $m$, $r$, $de$ and $k$ stand
    for non-relativistic species (baryons, cold dark-matter and non-relativistic neutrinos), 
    relativistic species (photons and relativistic neutrinos), dark-energy and curvature, 
    respectively. The curvature ``density'' parameter is defined as a function of the others,
    $\Omega_k = (1-\sum_x \Omega_x)$, and it is zero in a flat space geometry.
    Non-relativistic species dilute in an expanding universe so their energy density is 
    proportional to $a^{-3} = (1+z)^3$. Relativistic species dilute as well but have an
    extra factor of $a^{-1}$ due to redshifting. 
    Dark-energy is written as a general function of redshift. 
    In the simplest case of a cosmological constant, $\Omega_{de}(z) = \Omega_\Lambda$, i.e. a constant.
    The dependency of curvature with $a^{-2}$ simply comes from the field equations. 

    \subsection{Distances in cosmology}
    \label{intro:model:distances}

    Distances are non-trivially defined in an expanding, potentially non-flat, Universe. 
    Therefore, different types of distances are more appropriate for certain observables.
    The \emph{Hubble distance} is related to the expansion rate as 
    \begin{equation}
        D_H(z) = \frac{c}{H(z)},
        \label{eq:hubble_distance}
    \end{equation}
    and it is commonly used in measurements of baryon acoustic oscillations in the line-of-sight 
    direction (see Chapters~\ref{chap:forests} and \ref{chap:galaxies}) or to represent the
    size of a causally connected region of the Universe. 
    
    The \emph{comoving distance} to 
    an object at redshift $z$ is written as
    \begin{equation}
        D_C(z)  = \int_0^z \mathrm{d}z' ~ D_H(z').
        \label{eq:comoving_distance}
    \end{equation} 
    This expression yields the distance travelled by a photon from a source towards us but 
    factoring out the scale factor, effectively ``removing'' the effect of expansion. 
    
    The \emph{comoving angular diameter distance} $D_M(z)$ is useful when distances are inferred from 
    angles, which are affected by the curvature of space. 
    An object with comoving size $l$ at a redshift $z$ would be seen with an angular size $\theta$.
    This allows us to define $D_M(z) = l/\theta(z)$ as
    \begin{equation}
        D_M(z)  = D_C(z) \mathrm{sinc} \left(\sqrt{-\Omega_k} \frac{D_C(z)}{D_H(z=0)}\right),
        \label{eq:comoving_ang_diameter_distance}
    \end{equation}
    where $\mathrm{sinc}(x) = \sin(x)/x$. This distance is used in observations of 
    graviational lensing and in measurements of the baryon acoustic oscillations 
    in the transverse direction to the line-of-sight. 
    
    Analogouly, the \emph{luminosity distance} $D_L(z)$ 
    relates the flux $f$ received by an object at redshift $z$ with intrinsic luminosity $L$.
    We define then $D_L(z) = \sqrt{L/4\pi f(z)}$ as
    \begin{equation}
        D_L(z)  = D_M(z) (1+z).
        \label{eq:luminosity_distance}
    \end{equation}
    This distance is used in supernova cosmology, where fluxes are used to infer 
    dark-energy properties. Since supernovae fluxes vary by orders of magnitude, 
    it is handy to define the \emph{distance modulus} as the logarithm of the 
    luminosity distance:
    \begin{equation}
        \mu(z) = 5\log_{10}\frac{D_L(z)}{10~\mathrm{pc}}.
        \label{eq:distance_modulus}
    \end{equation}
    This definition is such that it agrees with the definition of magnitude of an object.

    In all the distances defined above, the dependency to the cosmological energy densities
    $\Omega_x$ happens only through $H(z)$ (Eq.~\ref{eq:expansion_rate}). This is how we can 
    constrain these parameters through distance measurements. However, all cosmological 
    distance measurements are \emph{relative}, which means they are defined to an arbitrary 
    normalisation\footnote{Two exceptions are parallax distances which are an absolute measurements
    based on the known distance between the Earth and the Sun and gravitational wave distances,
    which amplitudes depend on the distance and on the masses of the progenitors, though
    the masses affect the wave form as well, breaking the degeneracy.}. 
    It is only by observing the \emph{evolution} 
    of these relative distance measurements as a function of redshift 
    that we can constrain cosmological parameters.
    Note that in all the distances defined above, the dependency with $H_0$ only impacts
    this arbitrary normalisation, so it cannot be measured directly. 
    It is useful to factor out this dependency with $H_0$, and write distances 
    in units of $h^{-1}$Mpc (numerically equivalent to set $H_0 = 100$~km/s/Mpc). 
    These are the units used in all works studying the large-scale structures and it is 
    also used thoughout this manuscript. 
    
    \subsection{Dark-energy models}
    \label{intro:model:dark_energy_models}

    In the Eq.~\ref{eq:expansion_rate} for the expansion rate of the Universe, 
    the dark-energy density term is written as a general function of redshift.
    As mentioned before, the simplest model is to consider a cosmological constant
    $\Omega_{de}(z) = \Omega_\Lambda$.  
    The cosmological constant can be thought as equivalent to a fluid with 
    relativistic pressure $p_{de} = -\rho_{de}$. This model can be extended by 
    considering a different equation of state $p_{de} = p_{de}(\rho_{de}) = w(z) \rho_{de}$, 
    where $w(z)$ can be a constant or a more general function of redshift, under the
    condition that $w(z) < -1/3$, needed to obtain an accelerated expansion in a dark-energy 
    dominated Universe. One widely known parametrisation is given by $w(a) = w_0 + w_a(1-a)$ 
    (\cite{chevallierAcceleratingUniversesScaling2001, linderExploringExpansionHistory2003}). 
    
    The litterature contains a huge variety of models that attempt to be more physically motivated
    that those just presented (see \cite{weinbergObservationalProbesCosmic2013}, section 2.2 for a review). 
    Some of them suggest that maybe general relativity breaks down on large enough scales, 
    causing the expansion to accelerate. They suggest extensions or modifications to Einstein's 
    theory of general relativity, and are commonly known as \emph{modified gravity} models.   




\section{Model of the large-scale structures}
\label{intro:lss}

    The Universe is clearly not homogenous and isotropic. 
    Matter clustered under the influence of gravity, creating the cosmic web,
    composed of clusters, filaments, empty regions. 
    The model described in the previous section only describes the Universe
    as a whole, without any inhomogeneities. If we want to describe 
    the evolution of the structures we need to consider an extension to
    the background model. 

    Observations of the large-scale structures and  
    the cosmic microwave background (CMB)
    indicate that the matter density field today is the evolution of 
    tiny density perturbations from the early Universe. 
    Temperature fluctuations in the CMB are of the order of $10^{-5}$, 
    while densities today can reach values several orders of magnitude 
    larger than the average density.
    The most widely accepted theory is that the CMB fluctuations,
    and today's structures, are originated
    from the inflation-grown quantum fluctuations, that evolved under the 
    influence of gravity and pressure in the hot dense plasma-like epoch of our Universe. 
    
    It is been possible to model the physics of the evolution of these tiny 
    fluctuations using \emph{linear perturbation theory}. The predictions of this 
    theory are an excellent match to observations of the CMB and to the late-time
    distribution of matter on large scales, i.e., larger than few tens of Mpc. 
    On smaller scales however, gravity becomes highly non-linear and more advanced 
    calculations are required. There are two main approaches to model non-linearities:
    theoretical calculations going beyond linear terms in perturbation theory 
    or numerical n-body simulations. [Develop?]

    The main idea of perturbation theory is to consider that
    the density of a given species $x$ is given by:
    \begin{equation}
        \rho_x(\vec{x}, t) = \bar{\rho}_x(t)\left[1 + \delta_x(\vec{x}, t)\right],
    \end{equation}
    where the bar indicates average over the whole Universe and 
    $\delta_x \ll 1$ is a small perturbation. The average density 
    $\bar{\rho}_x$ follows the background evolution described in 
    section~\ref{sec:model_expanding_universe}. A new set of equations 
    can be derived for the evolution of $\delta_x$ for each species 
    depending if $x$ denotes relativitic species (photons, hot baryons, hot neutrinos) 
    or non-relativistic species (cold baryons, cold neutrinos or dark-matter). 
    Only some families of models also consider fluctuations in the dark-energy density.

    The evolution of $\delta_x$ for a given species is dictated by both 
    Boltzmann and Einstein's GR equations. Boltzmann equations describe 
    the evolution of phase-space distributions of each constituent, considering
    collisions (pressure) and particle creation and anihilation. GR equations
    describe how each species influence graviational potentials which in turn 
    make perturbations grow.

    Another important ingredient in these equations is the 
    \emph{peculiar velocities} $\vec{v}_x(\vec{x}, t)$ of 
    each species, particularly the non-relativistic ones since 
    relativistic species are assumed to have $v \sim c$. 
    They influence the evolution of perturbations through Boltzmann, Euler
    and continuity equations. These velocities are observable and are an 
    important probe of the cosmological model and on the strength of 
    gravity itself. They will be a key actor in Chapters~\ref{chap:galaxies} and 
    \ref{chap:velocities}. 
    
    \subsection{Statistical description of perturbations}
    \label{intro:lss:statistics}

    The value of the matter density field at a given position $\vec{x}$ is 
    hardly observable and therefore does not contain much cosmological 
    information, since it is just the evolution of a single random 
    realisation of the initial conditions. In cosmology we are mostly 
    interested by the statistical properties of the density field, or 
    of its perturbations. Particularly, most cosmological information is
    contained in the first moments of the density field, starting from the
    two-point function (the one-point function is zero by definition),
    which is defined as
    \begin{equation}
        \xi(\vec{x}, \vec{y}) = \langle \delta(\vec{x}) \delta(\vec{y})\rangle 
        \label{eq:two_point_function_config_space}
    \end{equation}
    where $\langle \cdot \rangle$ denotes an ensemble average. Given that
    we only have one single realisation of the Universe, we assume that
    averages over space (in an infinite space) are equivalent to ensemble averages.

    The assumptions of homogeneity and isotropy of the background can be 
    applied in an statistical sense to the perturbations. Statistical 
    homogeneity makes $\xi$ not to depend on the specific locations $\vec{x}$
    and $\vec{y}$ but only on their separation $\vec{r} = \vec{y}-\vec{x}$. 
    Statistical isotropic makes $\xi$ no longer depend on the orientation of
    $\vec{r}$, only on its absolute value $r = |\vec{r}|$. 
    Therefore, the two-point correlation function simplifies to a function of $r$:
    \begin{equation}
        \xi(r) = \langle \delta(\vec{x}) \delta(\vec{x}+\vec{r})\rangle. 
        \label{eq:two_point_function_config_space_iso_homo}
    \end{equation}
    
    The correlation function $\xi(r)$ is a powerful observable which can 
    be compared to predictions from a given cosmological model. 

    
    \subsection{Configuration and Fourier space}
    \label{intro:lss:config_fourier}

    The time evolution of linear density perturbations $\delta(\vec{x})$ 
    is dictated by a set of second-order differential equations, containing 
    derivatives with respect to time and space. 
    It is thus convient to apply Fourier
    transforms to these equations, where derivatives with respect to space 
    become simple products. Separations $\vec{r}$ in the so-called 
    \emph{Configuration space} become wavevectors $\vec{k}$ in 
    \emph{Fourier space}. The density perturbations in Fourier space are
    defined by 
    \begin{equation}
        \tilde{\delta}(\vec{k}) = \int \mathrm{d}^3x \ \mathrm{e}^{- i \vec{k}\cdot\vec{x}} \delta(\vec{x}),
    \end{equation}
    and its inverse is
    \begin{equation}
        \delta(\vec{x}) = \frac{1}{(2\pi)^3}\int \mathrm{d}^3k \ \mathrm{e}^{+i \vec{k}\cdot\vec{x}} \tilde\delta(\vec{k}).
    \end{equation}

    As previously, we are interested in the two (or more) point functions of the
    field. In Fourier space, the two-point correlation function is known as the
    \emph{power spectrum} and is defined as
    \begin{equation}
        \left\langle \tilde\delta(\vec{k}) \tilde\delta(\vec{k})' \right\rangle = (2\pi)^3 P(k) \delta^3_\mathrm{Dirac}(\vec{k}-\vec{k}'),
        \label{eq:power_spectrum_definition}
    \end{equation}
    where $\delta^3_\mathrm{Dirac}(\vec{x})$ is a three-dimensional Dirac distribution.
    The assumption of spatial homogeneity is guaranteed by the properties of Fourier space,
    while the assumption of isotropy makes the power spectrum to be only a function of the
    absolute value of the wavevector $k = |\vec{k}|$. 

    When considering the evolution of perturbations in Fourier space to the linear level,
    each mode $\tilde\delta(\vec{k})$ evolves independently of the others. We often say 
    that there is no \emph{mode coupling} in linear theory. 

    The power spectrum $P(k)$ is an observable, as powerful as the correlation function,
    to constrain cosmological models. While in theory these two contain exactly the
    same compressed statistical information, 
    in pratice the analyses of real data  
    use different finite ranges of scales 
    which are affected differently by 
    systematic effects. 
    Therefore, analyses of real data in configuration and Fourier space 
    complement each other. 
    Such analyses are presented in Chapter~\ref{chap:galaxies}. 

    \subsection{Cosmological dependency of the power spectrum}
    \label{intro:lss:cosmology_dependency}

    The power spectrum $P(k)$, or the correlation function $\xi(r)$, of density 
    perturbations are an excellent probe of the cosmological model. 
    It encodes information accumulated of the whole Universe's history, since the Big Bang
    until today.

    At the end of inflation, we think that perturbations were roughly Gaussian 
    (as are quantum fluctuations of the vacuum), 
    having a nearly scale independent power spectrum defined by
    \begin{equation}
        P_0(k) = A_s k^{n_s-1},
        \label{eq:initial_power_spectrum}
    \end{equation}
    where $A_s \sim 2 \times 10^{-9}$ is the amplitude of scalar perturbations 
    and $n_s \sim 0.96$ is the scalar spectral index. The term scalar refers to 
    standard density (or gravitational potential) perturbations, 
    while tensorial perturbations (of the space-time metric) refer to 
    primordial gravitational waves. Vectorial perturbations decay and rapidly become
    negligible in most common cosmological models. 

    After inflation, the majority of the energetic budget of Universe was held
    by photons and neutrinos. Since their energy density decays as $a^{-4}$, 
    their contribution quickly drops. This radiation-dominated era lasted until 
    $z \sim 5000$, or $t \sim 20$~kyr, when the energy density of matter became 
    the dominant source. Matter density decays at a slower rate, proportional to $a^{-3}$.
    Dark-energy became dominant at $z \sim 0.5$, or $t \sim 9$~Gyr, since its 
    energy density is constant (or close to constant).  
    The actual duration of each era depends on relative values of 
    the energy densities, parametrised by $\Omega_x$ 
    (see \ref{sec:model_expanding_universe}). 

    \textbf{ Add figure}
    Matter perturbations grew at different rates during these different eras.
    In radiation-dominated era, large-scale modes (large values of $r$ 
    or small values of $k$) grew while small-scale ones oscillated,
    due to battle between gravity and radiative pressure. 
    These are known as \emph{baryon acoustic oscillations}. These oscillations
    are imprinted in the power spectrum of the photon-baryon fluid, at the time of the CMB, 
    and in the matter power spectrum at later times. 
    The radiative pressure prohibited the growth of small-scale perturbations,
    so the matter power spectrum is damped, which results in this hill-like shape. 
    In the matter-dominated era, the radiative pressure becomes negligible 
    and all modes grow equally (neglecting non-linear growth). 
    When dark-energy is dominant, the expansion accelerates and 
    growth of structures slows down slightly. 
    
    
    Well after recombination,
    matter (baryons + dark) density perturbations obey the following differential equation, if we 
    assume GR and linear theory:
    \begin{equation}
        \ddot\delta(\vec{x}, t) + 2H(z)\dot\delta(\vec{x}, t) - \frac{3}{2} \Omega_m H_0^2 (1+z)^3 \delta(\vec{x}, t) = 0. 
        \label{eq:growth_equation}
    \end{equation}

    If we assume that $\delta(\vec{x}, t) = \delta(\vec{x}, t_0) G(t)/G(t_0)$, we can factor out 
    the spatial dependency and only solve for the time-dependent term $G(t)$, known as the 
    \emph{growth factor}. Its logarithmic derivative with respect to the scale factor $a(t)$ 
    is called \emph{growth rate} of structures:
    \begin{equation}
        f(a) \equiv \frac{ \mathrm{d}~ \ln G(a)}{\mathrm{d}~ \ln a},
        \label{eq:growth_rate}
    \end{equation}
    which is a key observable for which the predicted value 
    depends on the cosmological parameters and on the assumed
    theory of gravity (e.g., GR in Eq.~\ref{eq:growth_equation}). 

    In linear theory, peculiar velocities are simply related to 
    density perturbations via the continuity equation 
    \begin{equation}
        \nabla \cdot \vec{v}(\vec{x}, a) 
            \equiv \theta(\vec{x}, a) 
            = -a^2 H(a) \frac{\mathrm{d}\delta(\vec{x}, a)}{\mathrm{d}a} 
            = -a H(a) f(a) \delta(\vec{x}, a) 
        \label{eq:velocity_from_density}
    \end{equation}
    where $\theta$ is the velocity divergence, a convenient scalar field describing velocities. 
    In Fourier space, the velocity field is simply proportional to the density. 
    The velocity divergence power spectrum $P_{\theta \theta}$ is an important ingredient 
    for models of redshift-space distortions, as we discuss in the following section. 

    \subsection{The amplitude of the power spectrum}
    \label{intro:lss:amplitude_power_spectrum} 
    
    The matter power spectrum $P(k)$ represents the variance of density perturbations 
    at a given wavenumber $k$. The total variance of the density field is therefore 
    the sum of all contributions over all available three dimensional modes. 
    This is equivalent to the correlation function at zero separation:
    \begin{equation}
        \sigma^2 = \xi(r=0) = \frac{1}{(2\pi)^3}\int \mathrm{d}^3k ~ P(k) = \frac{1}{2\pi^2} \int_0^\infty \mathrm{d}k ~k^2 P(k), 
        \label{eq:variance_linear_field}
    \end{equation} 
    which diverges if we use the linear prediction for the matter power spectrum.
    This is because the linear power spectrum is the evolution of a nearly scale 
    invariant power spectrum, equivalent to white noise, so the variance simply 
    keeps increasing as we consider larger ranges of scales. Therefore, it is 
    convenient to smooth the linear density field using a three-dimensional 
    top-hat filter with radius $R$. The variance of the smoothed field is 
    then 
    \begin{equation}
        \sigma_R^2 = \frac{1}{2\pi^2} \int_0^\infty \mathrm{d}k ~k^2 P(k) W_R^2(k),
        \label{eq:variance_linear_field_smoothed}
    \end{equation}
    where $W_R(k) = 3[\sin(kR) - kR \cos(kR)]/(kR)^3$ is the Fourier transform of the
    top-hat filter. 

    In studies of large-scales structures, it is common to use $\sigma_R$ 
    as a parameter defining the amplitude of the linear power spectrum. 
    As discussed before, this amplitude depends on the values of $A_s$ (Eq.~\ref{eq:initial_power_spectrum})
    and $h$ (which determines the age of the Universe and how long structures could have grown).
    Therefore $\sigma_R$ is degenerate with $A_s$ and $h$ though is more representative of
    the amplitude of $P(k)$ at later times.  
    Other energy density parameters $\Omega_x$ also affect this amplitude, but these
    also change the overall shape and are not degenerate with $\sigma_R$. 

    The variance $\sigma_R$ is a decreasing function of $R$. Historically, 
    the chosen values for $R$ give variances near unity, which correspond to 
    the regime where linear theory should break.   
    Recently, the value of $R = 8h^{-1}$Mpc has been used in several analysis, 
    though it has been argued that chosing $R = 12$~Mpc (without the $h$ dependency)
    is a better choice to break degeneracies (\cite{sanchezArgumentsUsingMpc2020}).

    In measurements of redshift-space distortions and peculiar velocities, 
    there is a degeneracy between the value of $f$ and the
    amplitude of the power spectrum, $\sigma_8$. Therefore, 
    measurements can only constrain the combination $f\sigma_8$. 



    

\section{Cosmological probes of expansion}
\label{intro:probes}

    In this section, I present the basics of the cosmological 
    observables that allow us to learn about dark-energy. 
    Some of these are mostly sensitive to the background evolution
    presented in section~\ref{sec:model_expanding_universe}, while others 
    depend on the matter perturbations and their statistical properties as 
    discussed in section~\ref{sec:model_perturbations}. 

    \subsection{Direct measurements of \texorpdfstring{$H_0$}{the Hubble constant}}
    \label{intro:probes:h0}

    The Hubble constant $H_0$ is probably one of the first cosmological parameters 
    to be estimated by the one giving its name (\cite{hubbleRelationDistanceRadial1929}).
    Today, there are a few techniques to estimate $H_0$ which yield roughly independent results.
    See \cite{riessExpansionUniverseFaster2020} for a quick review of the latest results.  

    The most traditional method is known as the \emph{distance ladder}. 
    The idea is to measure the distance-redshift relationship of objects 
    well in the Hubble flow. 
    The distance to the closest objects in the Solar neighbourhood can 
    be estimated with the parallax method. The Gaia satellite has the current largest catalogue 
    of parallax measurements, of more than a billion stars. 
    Parallax is one of the few direct distance estimating methods. They can be used to 
    measure the intrinsic luminosity of some objects thought to be standard candles, such as 
    Cepheid stars, RR Lyrae, and the largest red giant stars. 
    Distances to farther objects can be estimated using these candles. They can also
    be used to calibrate other brighter standard candles, such as type-Ia supernovae, which 
    can be observed at the largest distances. 
    \cite{riessCosmicDistancesCalibrated2021a} contains the latest measurements using 
    Gaia parallaxes, Cepheids and type-Ia supernovae 
    to determine $H_0 = 73.0 \pm 1.4$~km/s/Mpc.  
    \cite{freedmanCarnegieChicagoHubbleProgram2019}
    is latest measurement of distances using the tip of the red giant branch. 
    
    The distance to the galaxy NGC 4258 could be determined thanks to the presence of a 
    water maser orbiting the center of this galaxy. The proper motions of several
    clouds orbiting close to the central massive black hole could be measured both 
    in radial and angular directions, strongly constraining the dynamics of the system
    (\cite{herrnsteinGeometricDistanceGalaxy1999b}).
    The most recent measurement yields $D = 7.576\pm0.082$~(stat.)~$\pm0.076$~(sys.)~Mpc
    (\cite{reidImprovedDistanceNGC2019}), which can be used as an alternative
    to Cepheids to anchor the 
    distance ladder and provide an estimate to $H_0$. 


    Another alternative method is based on the time-delay of the signals emitted from 
    a quasar behind a strong gravitational lens. The lens creates multiple images of the
    same background quasar, but the light paths have slightly different lengths. 
    Since quasars are variable objects, the same variability is observed with a delay of 
    several days between the different images. 
    By modelling the distribution of matter between the quasar and us, 
    it is possible to convert these time delays into an estimate of $H_0$.
    The ``$H_0$ Lenses in COSMOGRAIL's Wellspring'', or \textsc{H0LiCOW}, collaboration 
    produced the latest comprehensive measurement of $H_0$ using strongly-lensed quasars
    (\cite{wongH0LiCOWXIIICent2019}). 
    An alternative measurement has been performed using strong lenses from the Sloan (SLACS). 
    They are compared to \textsc{H0LiCOW} in \cite{birrerTDCOSMOIVHierarchical2020}. 
    
    More recently, gravitational waves from a merger of two black holes were 
    observed for the first time by the LIGO collaboration
    (\cite{theligoscientificcollaborationObservationGravitationalWaves2016}). 
    Not long after, a merger of two neutron stars was observed by both LIGO and Virgo,
    but this time an electromagnetic counterpart was also detected
    (\cite{theligoscientificcollaborationGW170817ObservationGravitational2017}), 
    pointing to the galaxy where the event occurred. Given the well predicted 
    shape of the wave form and its dependency on the masses of the neutron stars,
    it was possible to estimate the distance to the host galaxy. Combining with 
    a measurement of its redshift, a single event could provide an estimate of 
    $H_0$ (\cite{abbottGravitationalwaveStandardSiren2017b}). This opened the 
    field of cosmology using standard sirens, i.e., gravitational wave sources. 

    The cosmic microwave background can yield an estimate of 
    $H_0$ but it is strongly degenerate with other unknown parameters, such as curvature
    or dark-energy densities. Some of these degeneracies are reduced when considering 
    the effect of gravitational lensing of the CMB or when combining with other probes
    of late times. Therefore, precise measurements of $H_0$ from the CMB alone are only 
    possible when considering more restrictive models, such as a flat 
    space with a cosmological constant. 

    \subsection{Type-Ia supernovae}
    \label{intro:probes:snia}

    As mentioned earlier, type-Ia supernovae (SNIa) can be used as standard candles for 
    absolute distance measurements, if they are anchored by another method. 
    The SNIa can also be used alone to produce relative distance measurements and 
    constrain dark-energy. In this case, no anchor is required and the only assumption is
    that the SNIa intrinsic brightness does not evolve with redshift. 
     
    The main observable are the fluxes of SNIa as a function of time in different 
    photometric bands, known as light-curves. Spectroscopic follow up observations of 
    the explosion can confirm the type of the supernova based on the features
    present in their spectra. Another key ingredient is the redshift of the host 
    galaxy of the SNIa, which is commonly measured with spectroscopy as well.
    
    Spectro-photometric models of SNIa are used to fit the observed 
    light-curves, yielding their apparent magnitudes 
    at peak luminosity in a given photometric band. 
    These models account for correlations between color and duration of light curves 
    and the peak magnitude, reducing the intrinsic scatter in these magnitudes from 
    40\% to roughly 15\%. 
    It is essential to obtain accurate and precise measurements 
    of SNIa fluxes in order to obtain the best cosmological constraints. 
    Large sets of realistic simulations of the data are required in order to 
    correct for selection effects. The final product of the analysis is a set
    of distance moduli (Eq.~\ref{eq:distance_modulus}) and their host-galaxy redshift, 
    which can then be compared to models of expansion of the Universe. 
    
    Distance moduli depend on dark-energy through the integral of $H^{-1}(z)$. 
    In order to obtain good constraints on dark-energy properties, it is 
    important to have a large redshift coverage, at least covering the 
    transition between matter-dominated to dark-energy-dominated eras, so 
    $0< z< 1$. The advantage of SNIa for dark-energy studies is that they 
    can span these redshifts with a high sampling rate, which helps in 
    the study of the expansion-rate. One of the inconveniences is that SNIa 
    are complex and poorly understood astrophysical events, the intrinsic 
    scatter in luminosity cannot be reduced to better than 12\%, potentially 
    limiting the gains constraining power from future experiments. 
     
    The latest comprehensive study of SNIa combines data from more than a dozen 
    projects into a single sample, the Pantheon sample, from which dark-energy 
    constraints were derived (\cite{scolnicCompleteLightcurveSample2018}).
    The Dark Energy Survey also measured a more recent sample of few hundreds of SNe
    (\cite{broutFirstCosmologyResults2019a, broutFirstCosmologyResults2019, 
    kesslerFirstCosmologyResults2019}), 
    but not all of them have spectroscopic confirmation that they are of type Ia. 
    Their constraints on dark-energy are not yet competitive compared to the Panthon sample. 
    The Zwicky Transient Facility (ZTF, \cite{grahamZwickyTransientFacility2019}) 
    is currently observing the Northern sky 
    on a search for transient events and is expected to discover around 5000 
    spectroscopically confirmed SNIa at low redshifts ($0< z< 0.12$) until 2023. 
    After 2023, the Rubin Observatory Legacy Survey of Space and Time (LSST) 
    will take over in the Southern hemisphere
    and will discover more than 300 thousand SNIa up to $z< 0.5$. 
    
    While ZTF and Rubin's samples of SNIa will not decrease significantly the 
    errors on dark-energy parameters, they cover a redshift range where other 
    powerful probes, such as BAO or weak-lensing, lack of statistical power. 
    Furthermore, at lower redshifts when dark-energy is dominant, SNIa are 
    complementary to redshift-space distortions (RSD) when it comes to testing 
    the validity of General Relativity or constraining altenate models of gravity,
    as solutions for dark-energy. 

    Not only SNIa are a great probe of the 
    expansion history, they can also provide peculiar velocities of their 
    host galaxies via their inferred distances. 
    These peculiar velocities and their statistical properties can complement 
    RSD analysis when measuring the growth-rate of structures 
    (Eq.~\ref{eq:growth_rate}). Estimates of $H(z)$ and $f(z)$ can help 
    break degeneracies between simple dark-energy models and more involved 
    models of gravity (\cite{kimComplementarityPeculiarVelocity2020, 
    grazianiPeculiarVelocityCosmology2020}). 
    Chapter~\ref{chap:velocities} is dedicated to this 
    topic, to which I plan to dedicate the future years of my research.  



    \subsection{Big Bang nucleosynthesis}
    \label{intro:probes:bbn}

    In the post-inflation Universe, when temperatures
    are below the equivalent of 100 MeV, perturbations in the
    matter and radiation fields are quite 
    small and most of the physics is dictated by the interactions 
    between protons, electrons, neutrons, and photons,
    as described by the Boltzmann equations. 
    As the Universe cools down and rarefies, protons and neutrons 
    start forming atoms of deuterium, tritium, helium and heavier elements.
    This process is known as the Big Bang nucleosynthesis (BBN).

    The relative amount of each of the formed elements depends on the expansion rate 
    of the Universe at that time as well as the physical density of baryons 
    $\omega_b = \Omega_b h^2$ and radiation $\omega_r = \Omega_r h^2$.
    Given that the energies involved are within reach of particle 
    accelerators, these reactions can be studied with great detail on Earth,
    allowing us to build accurate models of the BBN 
    (see \cite{pitrouNewTensionCosmological2021} for the latest calculations
    and references therein). 
    
    Observations of the primordial abundances can be compared to the predictions 
    by BBN models. Abundances of deuterium, helium, and others can be estimated 
    from spectroscopic observations of HII regions in metal-poor galaxies 
    or from absorption lines of the intergalactic medium in quasar spectra. 
    The most up-to-date measurements of the primordial helium-4 abundance yields 
    $Y_p = 0.2453 \pm 0.0034$ (\cite{averImprovingHeliumAbundance2021}), while
    the deuterium one is $\mathrm{D/H} = (2.527 \pm 0.030) \times 10^{-5}$
    (\cite{pitrouNewTensionCosmological2021}). 
    While observations are consistent with BBN models for deuterium and helium, 
    the abundance of lithum-7 exhibits a factor 3 discrepancy, which is a huge problem 
    in BBN but quite often neglected. 
    
    The temperature fluctuations in the cosmic microwave background (CMB) are
    also very sensitive to the physical baryon density $\omega_b$. 
    Historically, the values obtained from CMB have been in good agreement 
    with BBN measurements, showing that baryons make up to around 16\% of the total 
    matter content of the Universe, or $\omega_b = (2.195 \pm  0.022)\times 10^{-2}$. 
    The agreement between two quite independent probes is one of the great successes 
    of the current cosmological model, thought they also enforce the need for a dark-matter 
    component.  

   
    \subsection{Baryon acoustic oscillations}
    \label{intro:probes:bao}

    Baryon acoustic oscillations (BAO) is the name given to the propagation of sound
    waves in the primordial plasma (baryons and photons), prior to recombination. 
    Because of the high pressure on small scales at those times, 
    each initial density perturbation had a spherical density wave around them 
    propagating outwards at the speed of sound in that medium. The speed of sound
    in the plasma is given by 
    \begin{equation}
        c_s(z) \equiv \sqrt{\frac{1}{3[1+R(z)]}}
        \label{eq:sound_speed}
    \end{equation}
    where $R(z) = 3\rho_b / 4 \rho_\gamma$ is the baryon-to-photon ratio.
    The propagation of sound waves occurred until the temperatures and densities 
    dropped to values such that baryons no longer 
    felt the pressure from photons, known as the \emph{drag epoch}, which is 
    close in time to the recombination (but technically not the same eopch). 
    This process left a slight overdense shell around each initial perturbation
    with a radius given by 
    \begin{equation}
        r_\mathrm{drag} \equiv r_s(z_\mathrm{drag}) = 
            \int_\infty^{z_\mathrm{drag}} \mathrm{d}z' \frac{c_s(z')}{H(z')}
        \label{eq:sound_horizon}
    \end{equation}
    which is known as the \emph{sound horizon at drag epoch}, or the BAO scale. 
    Today, $r_d$ has a physical size of about 147 Mpc, much larger than any 
    collapsed structure in the Universe. As one can see from Eqs.~\ref{eq:sound_speed}
    and \ref{eq:sound_horizon}, $r_\mathrm{drag}$ mainly depends on 
    $\omega_b$ and $\omega_c$ assuming the CMB gives a precise 
    estimate of $\omega_\gamma$. The dependency of $r_\mathrm{drag}$ 
    is mainly through $\omega_b$, which is useful 
    when constraining dark energy models for instance. 

    After recombination, the sound horizon scale only increases in size 
    due to the expansion, or equivalently, its comoving size remains unchanged. 
    Therefore, the BAO scale is a great \emph{standard ruler} to study the expansion 
    rate of the Universe. 
    In practice, the BAO scale is observed statistically in the 
    two-point function of the matter density field, so it is often 
    classified as an statistical standard ruler. 
    In configuration space, the correlation function $\xi(r)$ presents a small 
    peak at separations corresponding to the BAO scale $r_d$, while in Fourier space 
    the power spectrum $P(k)$ contains an oscillatory pattern as a function of scale 
    with a period proportional to the BAO scale.  
    In Chapters~\ref{chap:forests} and \ref{chap:galaxies}, I present 
    my past work in the measurement of the BAO scale using \lya\ forests 
    and galaxies, respectively, as tracers of the matter density field.
    
    Given that a galaxy survey is made of angular positions and redshifts, 
    which are true observables, 
    the BAO peak is effectively measured as an angle $\Delta \theta_\mathrm{BAO}$ 
    or as a difference in redshift $\Delta z_\mathrm{BAO}$. 
    These can be modelled as ratios of distances to the BAO scale $r_d$ as
    \begin{equation}
        \Delta \theta_\mathrm{BAO}(z_\mathrm{eff}) = \frac{D_M(z_\mathrm{eff})}{r_d}
        \label{eq:bao_angular}
    \end{equation}
    \begin{equation}
        \Delta z_\mathrm{BAO}(z_\mathrm{eff}) = \frac{D_H(z_\mathrm{eff})}{r_d}
        \label{eq:bao_radial}
    \end{equation}
    where $z_\mathrm{eff}$ is the effective redshift of the galaxy survey, 
    $D_M$ is the comoving angular diameter distance (Eq.~\ref{eq:comoving_ang_diameter_distance}),
    and $D_H$ is the Hubble distance (Eq.~\ref{eq:hubble_distance}). 

    There are two ways BAO can be used to constrain cosmological models, 
    depending if we assume that $r_\mathrm{drag}$ is known or not. 
    If $r_\mathrm{drag}$ is known, i.e. given by Eq.~\ref{eq:sound_horizon} 
    using some value for $\omega_b$
    (given by the CMB or BBN for instance), then BAO measurements
    are converted to absolute distance measurements which depend only on  
    $H(z)$. Therefore, BAO can constrain $H_0$, curvature and dark energy. 
    If $r_\mathrm{drag}$ is supposed to be unknown but still a standard ruler, 
    then BAO constraints the ratio of $E(z) = H(z)/H_0$ to the combination 
    $H_0 r_\mathrm{drag}$ which is now degenerate. This is similar to 
    type Ia supernovae, where their absolute magnitude is degenerate with $H_0$.
    By combining several BAO measurements at different effective redshifts, 
    BAO is a powerful probe of dark energy and curvature. 

    Current BAO measurements span effective redshifts from 0.1 to 2.3.
    The Sloan Digital Sky Survey (SDSS, 
    \cite{eisensteinSDSSIIIMassiveSpectroscopic2011, blantonSloanDigitalSky2017}) 
    has measured more than 2 million 
    redshifts spectroscopically in the past twenty years, 
    producing the largest maps to date of the
    distribution of matter in the Universe. In addition to SDSS, 
    surveys such as FastSound (\cite{okumuraSubaruFMOSGalaxy2016}), 
    Vipers (\cite{pezzottaVIMOSPublicExtragalactic2017}),
    6 degree field galaxy survey (6dFGS, \cite{beutler6dFGalaxySurvey2012a})
    and WiggleZ (\cite{parkinsonWiggleZDarkEnergy2012}) also
    produced BAO measurements using galaxies, though the volumes 
    probed and the number of galaxies is inferior to SDSS. 
    The latest cosmological constraints from BAO are described in 
    \cite{alamCompletedSDSSIVExtended2021}.



    \subsection{The cosmic microwave background}
    \label{intro:probes:cmb}

    The cosmic microwave background (CMB) is one of the richest cosmological probes of all. 
    The photons we receive today last scattered on baryons at a redshift of about 1100,
    corresponding to roughly 380 000 years after the Big Bang. 
    It is the oldest information 
    that we can measure from the Universe today. 
    
    The average temperature $T_\mathrm{CMB} = 2.72548 \pm 0.00057$~K 
    (\cite{matherMeasurementCosmicMicrowave1994, fixsenTemperatureCosmicMicrowave2009}) 
    of these photons, measured the last time by the COBE satellite, tell us on how much 
    energy density from radiation there is in the Universe. From the black body Bose-Einstein 
    distribution of photons, we have 
    \begin{equation}
        \rho_\gamma = \frac{\pi^2 k^4}{15 \hbar^3 c^3} T_\mathrm{CMB}^4
        \label{eq:energy_density_photons}
    \end{equation} 
    
    The fluctuations around this average temperature, $\Delta T$, 
    and the polarisation of these photons
    trace the structures back at the recombination epoch. 
    Given its early-times nature, 
    the temperature and polarisation fields are extremely well described by 
    Gaussian statistics. Most of the information is therefore contained in the two point
    functions of these fields. Given that these fields are two dimensional, functions of
    the position angle in the sky $(\theta, \phi)$, it is convenient to decompose these 
    fields into a basis of spherical harmonic functions $Y_{\ell m}(\theta, \phi)$, 
    which amplitudes are denoted $a_{\ell m}$. The angular power spectrum  
    is simply the variance of these amplitudes: $C_\ell = \langle a_{\ell m} a^*_{\ell m}\rangle$, 
    which in linear theory is simply a function of $\ell$ and independent of $m$. 
    The temperature and both polarisation modes (E and B) can be decomposed into 
    spherical harmonics, so we can estimate all cross power spectra, e.g., 
    $C^{TE}_\ell = \langle a^T_{\ell m} a^{E*}_{\ell m}\rangle$ is the
    cross temperature and E-mode polarisation power spectrum. 
    
    The temperature and polarisation auto and cross power spectra 
    are exquisitely well modelled by linear perturbation theory. 
    In the most basic $\Lambda$CDM model, the power spectrum is a 
    function of only six parameters: the angular scale of baryon 
    acoustic oscillations $\theta_*$, 
    the physical density of baryons $\omega_b$ and dark-matter $\omega_c$, 
    the primordial power spectrum amplitude $A_s$ and slope $n_s$, 
    and the optical depth $\tau$. 
    The fact that such a model with so few free parameters can 
    describe so well the observations is one of the greatest achievements 
    in modern physics. A great description of the CMB physics 
    from first principles can be found in \cite{dodelsonscottModernCosmology2nd2020}.
    Several codes are available to compute models for the CMB 
    such as \textsc{CAMB}\footnote{\url{https://camb.info/}} 
    (\cite{lewisEfficientComputationCosmic2000})
    and \textsc{CLASS}\footnote{\url{https://lesgourg.github.io/class_public/class.html}} 
    (\cite{lesgourguesCosmicLinearAnisotropy2011}). 
    
    Explain how the power spectrum depends on the parameters
    
    Latest measurements: 
    Planck satellite (\cite{planckcollaborationPlanck2018Results2020}), 
    Atacama Cosmology Telescope (ACT, \cite{aiolaAtacamaCosmologyTelescope2020}), 
    South Pole Telescope (SPT, \cite{aylorComparisonCosmologicalParameters2017, 
    balkenholConstraintsLambdaCDM2021, dutcherMeasurementsEModePolarization2021}),
    BICEP/Keck (\cite{bicep/keckDemonstrationImprovedConstraints2021}) for B-modes. 
    




 

    \subsection{Redshift-space distortions}
    \label{intro:probes:rsd}

    Matter is not static in the Universe. 
    Due to gravity, matter flows from underdense regions towards overdense ones. 
    These velocities are commonly called \emph{peculiar velocities}.
    The radial component of peculiar velocities alters the observed redshift 
    of an object, due to the Doppler effect. 
    Therefore, the total observed redshift $z_\mathrm{obs}$
    is a combination of its cosmological redshift $z_\mathrm{cos}$, 
    due to the expansion of the Universe,
    and the peculiar redshift $z_\mathrm{pec}$, due to the Doppler effect:
    \begin{equation}
        1 + z_\mathrm{obs} = (1 + z_\mathrm{cos})(1 + z_\mathrm{pec})
        \label{eq:observed_redshift}
    \end{equation}
    where
    \begin{equation}
        z_\mathrm{pec} = \sqrt{\frac{1 + \frac{\vec{v}\cdot \hat{n}}{c} }{1 - \frac{\vec{v}\cdot \hat{n}}{c}}} - 1
                        \approx \frac{\vec{v}\cdot \hat{n}}{c}
        \label{eq:peculiar_redshift}
    \end{equation}
    The first equality is the definition of the relativistic Doppler effect, 
    $\vec{v}$ is the velocity and $\hat{n}$ is the radial unitary vector.  
    The right-hand side is the non-relativistic approximation, which is accurate
    for typical velocities of matter flows in our Universe.

    When converting observed redshifts into comoving distances, peculiar velocities
    introduce a small error such that 
    \begin{equation}
        s(z_\mathrm{obs}) \approx r(z_{\mathrm cos}) + \frac{(1+z_\mathrm{cos})}{H(z_\mathrm{cos})}\vec{v} \cdot \hat{n},
        \label{eq:redshift_space_distance}
    \end{equation}
    where $s$ is the \emph{redshift-space} distance and $r$ is the \emph{real-space} distance.
    The errors caused by peculiar velocities distort the observed distribution of matter/galaxies
    since we cannot in general decouple the velocities and the Hubble flow. 
    This effect is observable and named \emph{redshift-space distortions} (RSD). 
    RSD are a powerful probe of the dynamics of the matter field, i.e., 
    on how velocities are related to the densities, and how they contribute
    to the growth of structures over time. More generally, 
    observations of RSD can be used to test the validity of general 
    relativity since it is a probe of the strength of gravity. 

    The density contrast of matter is modified when observed in redshift-space relative 
    to real-space, given the redshift-space distortions. Mass conservation implies that
    \begin{equation}
        [1+\delta_s(\vec{s})] \dinteg^3s = [1+\delta(\vec{x})]\dinteg^3 x,
        \label{eq:mass_conservation}
    \end{equation}
    such that in Fourier space we obtain, in the linear regime 
    (\cite{kaiserClusteringRealSpace1987})
    \begin{equation}
        \delta_s(\vec{k}) = (1 + f \mu^2 )\delta(\vec{k})
        \label{eq:delta_rsd_fourier}
    \end{equation}
    where $f$ is the growth-rate of structures defined in Eq.~\ref{eq:growth_rate} 
    and $\mu = \vec{k}\cdot \hat{n} / k$ or the cosine of the angle between the wavevector 
    and the line of sight. The power spectrum in redshift-space assuming linear 
    perturbations is simply 
    \begin{equation}
        P_s(k, \mu) = (1 + f\mu^2)^2 P_m^\mathrm{lin}(k)
        \label{eq:power_spectrum_rsd_linear}
    \end{equation}
    
    This means that the redshift-space matter power spectrum has radial modes enhanced 
    by a factor of $(1+f)^2$ (where $f \sim 1$) relative to modes transverse to the line 
    of sight.  This enhancement is observable with galaxy surveys. If the normalisation 
    of the linear matter power spectrum is parametrised by $\sigma_8$ 
    (Eq.~\ref{eq:variance_linear_field_smoothed}), then the actual measured quantity is 
    the product $f\sigma_8$. Transposed to a given effective redshift $z_\mathrm{eff}$
    of a galaxy survey, we need to scale $\sigma_8$, which is usually defined at $z=0$, 
    using the growth factor $D(z)$, such that the measured quantity is $f(z)\sigma_8 D(z)$. 
    
    The observable $f(z)\sigma_8 D(z)$ is mostly sensitive to the amount of 
    dark matter $\omega_m$, which drives the growth of structures, 
    and $H(z)$, which damps the growth (see Eq.~\ref{eq:growth_equation}). 
    Therefore, it can be used to constrain dark energy models. More generally, 
    measurements of $f(z)\sigma_8 D(z)$ can be used to test the 
    the validity of general relativity on large scales, which is 
    the commonly assumed theory of gravity. 

    The latest most relevant measurements were performed using data from 
    the Sloan Digital Sky Survey (SDSS), including 
    \begin{itemize} 
        \item the SDSS Main Galaxy Sample \cite{howlettClusteringSDSSMain2015},
        \item the Baryon Oscillation Spectroscopic Survey (BOSS, \cite{alamClusteringGalaxiesCompleted2017}), 
        \item the extended BOSS luminous red galaxy sample (eBOSS LRG, \cite{bautistaCompletedSDSSIVExtended2020, gil-marinCompletedSDSSIVExtended2020}), 
        \item the eBOSS emission line galaxy sample (eBOSS ELG, \cite{tamoneCompletedSDSSIVExtended2020, demattiaCompletedSDSSIVExtended2021}),
        \item the eBOSS quasar sample (eBOSS QSO, \cite{houCompletedSDSSIVExtended2021, neveuxCompletedSDSSIVExtended2020}),
    \end{itemize}

    In chapter~\ref{chap:galaxies} I present my contributions to the measurement 
    of the growth-rate of structures using the eBOSS LRG sample 
    (\cite{bautistaCompletedSDSSIVExtended2020}).
    The cosmological implications of SDSS growth-rate measurements are described 
    in \cite{alamCompletedSDSSIVExtended2021}. 

    \subsection{Weak gravitational lensing}
    \label{intro:probes:wl}

    Photons follow space-time geodesics. Space-time is distorted in the presence 
    of a source of gravitational potential, which is typically in the form of a mass concentration. 
    Therefore, the matter distribution in the Universe bends photon trajectories from distant sources.  
    This phenomenon receives the name of \emph{gravitational lensing}, since the 
    theory describing light propagation on a gravitational field is analogous to classical optics.  
    Gravitational lensing is a rich cosmological probe since the distorion of photon trajectories
    depends on both the baryonic and dark matter. From lensing measurements we can learn 
    about the total matter distribution in an expanding Universe. 
    
    In the case of an extended source of photons, e.g., a galaxy, light from different 
    angular positions within the source take slighly different paths towards the observer. 
    If this extended source is placed in the background of a mass concentration - the lens - each 
    path will suffer a slightly different bending, which depends on the impact parameter 
    of each photon relative to the center of this lens. 
    Also, due to conservation of surface brightness (is this the correct name?), 
    the total flux is also increased. 
    The final result is an image shifted, distorted and brighter. 

    Lensing measurements use shifts, distortions and increase in flux to determine 
    properties of the lenses. These measurements are hard since we do not have access to 
    the original unlensed position, shape and flux of a given galaxy. 
    One remarkable exception is lensing in its strong regime. When the ratio between 
    impact parameter of the source and mass of the lens is below some threshold 
    - the Einstein radius - multiple images of the same source are created. 
    Strong lensing allows us to estimate the mass and density profile of a given lens. 
    Moreover, if the source is variable in time, this variability is slightly delayed 
    between each of the multiple images, which allows us to constrain the expansion 
    of the Universe. As mentioned in section~\ref{intro:probes:h0}, strongly lensed 
    quasars (variable sources) are used to constrain $H_0$, under the assumption 
    that we can properly model the lens density profile. 


    However, statistical measurements of lensing, such as correlations with overdensities, 
    can be performed by making assumptions on the statistical properties of the unlensed 
    sample of galaxies. 

    weak regime we measure shapes and correlate themselves (shear-shear) or with the galaxy field (galaxy-shear)

    Sensitive to: the amplitude of the shear power spectrum, $S_8 = \sigma_8 \sqrt{\Omega_m/0.3}$.

    Latest measurements: 
    Kilo Degree Survey (KiDS, \cite{heymansKiDS1000CosmologyMultiprobe2021}),
    Dark Energy Survey Year 3 (DES-Y3, \cite{descollaborationDarkEnergySurvey2021}), 
    Hyper Supreme Cam first year (HSC, \cite{hikageCosmologyCosmicShear2019})


